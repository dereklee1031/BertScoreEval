# LLM-Evaluation: Summarizing Meetings with GPT-4 and BERTScore

This project aims to evaluate the quality of meeting summaries generated by GPT-4 using BERTScore as the primary evaluation metric. The goal is to compare different prompt configurations and model settings to optimize the summarization process and achieve the best possible results.

## Table of Contents

- [Introduction](#introduction)
- [Project Structure](#project-structure)
- [Setup](#setup)
- [Running the Project](#running-the-project)
  - [Summarization](#summarization)
  - [Evaluation](#evaluation)
- [Results Logging](#results-logging)
- [Experiments](#experiments)
- [Future Work](#future-work)
- [Contributing](#contributing)
- [License](#license)
- [Contact](#contact)

## Introduction

This project explores the use of GPT-4 for summarizing meeting transcripts and evaluates the generated summaries using BERTScore. The focus is on optimizing the summarization by experimenting with different prompt structures and model parameters.

## Project Structure

```
LLM-Evaluation/
│
├── configs/
│   ├── firestore_config.json          # Firestore configuration file
│   ├── serviceAccountKey.json         # Firestore service account key
│   └── gcloud_service_credentials.json# Google Cloud service credentials
│
├── data/
│   ├── transcriptions_data.csv        # CSV file with meeting transcriptions and metadata
│   ├── multiple_summaries_openai.csv  # Generated summaries from GPT-4
│   ├── bert_score_results.csv         # BERTScore evaluation results
│   └── human_summaries.csv            # Human-written summaries for comparison
│
├── src/
│   ├── gbt4.py                   # Main script to generate summaries using GPT-4
│   ├── bert.py                    # Script to evaluate summaries using BERTScore
│   └── utils.py                       # Utility functions for text processing and chunking
│
├── Dockerfile                         # Dockerfile for containerizing the project
├── requirements.txt                   # Project dependencies
├── README.md                          # Project overview and setup instructions
└── .gitignore                         # Files to ignore in the repository
```

## Setup

### Prerequisites

- Python 3.8 or higher
- Access to OpenAI API (GPT-4)
- Google Cloud credentials (if using Firestore for data storage)

### Installation

1. **Clone the repository**:
   ```bash
   git clone https://github.com/dereklee1031/LLM-Evaluation.git
   cd LLM-Evaluation
   ```

2. **Install dependencies**:
   ```bash
   pip install -r requirements.txt
   ```

3. **Set up environment variables**:
   - Set your OpenAI API key in an environment variable or directly in the scripts.
   - Ensure your Google Cloud credentials are properly set up if you're using Firestore.

## Running the Project

### Summarization

To generate summaries from the meeting transcripts:

```bash
python src/summarize.py
```

This script loads the meeting transcripts, splits them into manageable chunks, and generates summaries using GPT-4.

### Evaluation

To evaluate the generated summaries using BERTScore:

```bash
python src/evaluate.py
```

This script compares the GPT-4-generated summaries with human-written summaries using BERTScore and logs the results.

## Results Logging

The results, including precision, recall, and F1 scores, are logged in a CSV file (`bert_score_results.csv`). Each experiment’s configuration (prompt, model parameters) is recorded to allow for systematic comparison and optimization.

## Experiments

To run experiments with different prompts and model settings, you can modify the `summarize.py` script or run multiple configurations programmatically. The results should be recorded in a structured log (e.g., `results_log.csv`) for comparison.

## Future Work

- **Model Fine-Tuning**: Explore fine-tuning GPT-4 with domain-specific data to improve summary accuracy.
- **Additional Metrics**: Integrate other evaluation metrics like ROUGE for a more comprehensive assessment.
- **Human Evaluation**: Incorporate human evaluations to supplement automated metrics.

## Contributing

Contributions are welcome! Please follow these steps:

1. Fork the repository.
2. Create a new branch for your feature or bugfix.
3. Submit a pull request with a clear description of your changes.

## License

This project is licensed under the MIT License. See the `LICENSE` file for more details.

## Contact

If you have any questions or suggestions, please feel free to reach out to me at dereklee921031@gmail.com

---
